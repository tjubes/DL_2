{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a944c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scipy -y\n",
    "# !conda install seaborn -y\n",
    "# !conda install sklearn-pandas -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09293f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m predictions, true_labels, test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(\u001b[43mnet\u001b[49m, testloader, criterion)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# Add these imports to your main.py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def test_model(net, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Test the model and return predictions, labels, and loss\"\"\"\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(testloader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    return all_predictions, all_labels, avg_test_loss, accuracy\n",
    "\n",
    "def analyze_results(predictions, true_labels, label_map, model_name=\"CNN\"):\n",
    "    \"\"\"Comprehensive analysis of model results\"\"\"\n",
    "    \n",
    "    # Reverse label mapping for display\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    label_names = [reverse_label_map[i] for i in range(len(label_map))]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {model_name} MODEL ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(\"-\" * 40)\n",
    "    report = classification_report(true_labels, predictions, \n",
    "                                 target_names=label_names, \n",
    "                                 digits=4)\n",
    "    print(report)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class analysis\n",
    "    print(f\"\\nPer-Class Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        true_positives = cm[i, i]\n",
    "        total_actual = np.sum(cm[i, :])\n",
    "        total_predicted = np.sum(cm[:, i])\n",
    "        \n",
    "        if total_actual > 0:\n",
    "            recall = true_positives / total_actual\n",
    "            precision = true_positives / total_predicted if total_predicted > 0 else 0\n",
    "            print(f\"{label_name:15s}: Precision={precision:.3f}, Recall={recall:.3f}, Count={total_actual}\")\n",
    "    \n",
    "    # Identify problematic classes\n",
    "    print(f\"\\nMost Confused Classes:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find highest off-diagonal values in confusion matrix\n",
    "    confusion_pairs = []\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(len(label_names)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_pairs.append((cm[i, j], label_names[i], label_names[j]))\n",
    "    \n",
    "    confusion_pairs.sort(reverse=True)\n",
    "    for count, true_class, pred_class in confusion_pairs[:5]:\n",
    "        print(f\"{true_class} â†’ {pred_class}: {count} misclassifications\")\n",
    "    \n",
    "    return accuracy, cm\n",
    "\n",
    "def plot_training_history(train_losses, train_accuracies=None):\n",
    "    \"\"\"Plot training history if you tracked it\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 if train_accuracies else 1, figsize=(12, 4))\n",
    "    \n",
    "    if train_accuracies:\n",
    "        axes[0].plot(train_losses)\n",
    "        axes[0].set_title('Training Loss')\n",
    "        axes[0].set_xlabel('Epoch/Batch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        \n",
    "        axes[1].plot(train_accuracies)\n",
    "        axes[1].set_title('Training Accuracy')\n",
    "        axes[1].set_xlabel('Epoch/Batch')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "    else:\n",
    "        axes.plot(train_losses)\n",
    "        axes.set_title('Training Loss')\n",
    "        axes.set_xlabel('Epoch/Batch')\n",
    "        axes.set_ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def model_summary(net):\n",
    "    \"\"\"Print model architecture summary\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  MODEL ARCHITECTURE SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Model Size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nLayer Details:\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, module in net.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Only leaf modules\n",
    "            num_params = sum(p.numel() for p in module.parameters())\n",
    "            if num_params > 0:\n",
    "                print(f\"{name:20s}: {str(module):50s} | {num_params:,} params\")\n",
    "\n",
    "# Usage - Add this after your training is complete:\n",
    "print(\"Testing the model...\")\n",
    "\n",
    "# Test the model\n",
    "predictions, true_labels, test_loss, test_accuracy = test_model(net, testloader, criterion)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Comprehensive analysis\n",
    "analyze_results(predictions, true_labels, LABEL_MAP, \"Intra-Subject CNN\")\n",
    "\n",
    "# Load the saved model\n",
    "net = MEGNet(num_classes=len(LABEL_MAP), \n",
    "             input_channels=248, \n",
    "             input_time_steps=train_data[0].shape[1])\n",
    "\n",
    "# Load the saved state dict\n",
    "net.load_state_dict(torch.load('CNN_1.pth'))\n",
    "\n",
    "# Set to evaluation mode for testing\n",
    "net.eval()\n",
    "\n",
    "print(\"Model loaded successfully from CNN_1.pth\")\n",
    "\n",
    "# Model summary\n",
    "model_summary(net)\n",
    "\n",
    "# If you tracked training history, uncomment and use:\n",
    "# plot_training_history(your_training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424aba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
