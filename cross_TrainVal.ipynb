{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b4e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_subject_train.py\n",
    "import numpy as np\n",
    "import os\n",
    "#from Utils.utilities import *\n",
    "from Utils.utilities_Cross import *\n",
    "import h5py\n",
    "from scipy.stats import zscore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18cdc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5227d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D_LSTM_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A hybrid deep learning model for MEG classification.\n",
    "\n",
    "    Architecture:\n",
    "    1. 2D CNN layers: Learn local spatiotemporal features from MEG sensor data.\n",
    "    2. LSTM layer: Captures global temporal dependencies across time.\n",
    "    3. Fully connected layers: Map learned features to class probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=4, input_sensors=248, input_time_steps=2227,\n",
    "                 lstm_hidden_size=64, num_lstm_layers=2, dropout=0.4):\n",
    "        super(CNN2D_LSTM_Net, self).__init__()\n",
    "\n",
    "        self.input_sensors = input_sensors\n",
    "        self.input_time_steps = input_time_steps\n",
    "\n",
    "        # === Step 1: 2D CNN over (sensors x time) ===\n",
    "        # This block extracts local patterns in both space (sensor layout) and time.\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Conv Layer 1: kernel, stride reduces resolution\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 11), stride=(1, 2), padding=(1, 5)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2)),  \n",
    "            nn.Dropout2d(dropout),\n",
    "\n",
    "            # Conv Layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 7), stride=(1, 2), padding=(1, 3)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2)),  \n",
    "            nn.Dropout2d(dropout)\n",
    "        )\n",
    "\n",
    "        # === Auto-calculate CNN output size for LSTM ===\n",
    "        # Forward a dummy input to determine the dimensions of the CNN output\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_sensors, input_time_steps)\n",
    "            cnn_out = self.cnn(dummy_input)\n",
    "            _, cnn_channels, reduced_sensors, reduced_time = cnn_out.shape\n",
    "\n",
    "            # The LSTM will receive one feature vector per time step,\n",
    "            # with size = channels × reduced spatial dimension\n",
    "            self.lstm_input_size = cnn_channels * reduced_sensors\n",
    "            self.sequence_length = reduced_time\n",
    "\n",
    "        # === Step 2: LSTM to model temporal dynamics ===\n",
    "        # Bidirectional LSTM captures temporal dependencies in both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_lstm_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        # === Step 3: Classifier (fully connected layers) ===\n",
    "        # Maps LSTM output to final class scores\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, lstm_hidden_size//2),  # bidirectional = 2×hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_hidden_size//2, num_classes)\n",
    "        )\n",
    "\n",
    "        # === Print model summary ===\n",
    "        print(f\"2D CNN + LSTM Net initialized:\")\n",
    "        print(f\"  - LSTM input size: {self.lstm_input_size}\")\n",
    "        print(f\"  - Sequence length: {self.sequence_length}\")\n",
    "        print(f\"  - Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, 1, sensors, time_steps)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # === Step 1: 2D CNN ===\n",
    "        x = self.cnn(x)  # Output shape: (B, channels, reduced_sensors, reduced_time)\n",
    "\n",
    "        # === Step 2: Prepare for LSTM ===\n",
    "        # Permute and flatten spatial dimensions: (B, time, features)\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (B, reduced_time, channels, reduced_sensors)\n",
    "        x = x.contiguous().view(batch_size, self.sequence_length, -1)\n",
    "\n",
    "        # === Step 3: LSTM ===\n",
    "        lstm_out, _ = self.lstm(x)  # -> (B, time, 2 * hidden_size)\n",
    "\n",
    "        # === Step 4: Classification using final time step ===\n",
    "        x = lstm_out[:, -1, :]  # Extract features from the last time step\n",
    "\n",
    "        return self.classifier(x)  # -> (B, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "415f9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base filepath: c:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\Final Project data\\Final Project data\n"
     ]
    }
   ],
   "source": [
    "# Use os.path.join() to create the correct file path\n",
    "filepath = get_filepath()\n",
    "print(f\"Base filepath: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61df47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same label mapping and model architecture as main.py\n",
    "LABEL_MAP = {\n",
    "    'rest': 0,\n",
    "    'task_motor': 1, \n",
    "    'task_story': 2,\n",
    "    'task_working': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eeef940",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model hyperparameters\n",
    "    \"num_classes\": len(LABEL_MAP),\n",
    "    \"input_sensors\": 248,\n",
    "    \"input_time_steps\": 8906,\n",
    "    \"lstm_hidden_size\": 32,\n",
    "    \"num_lstm_layers\": 1, #1 or 2\n",
    "    \"dropout\": 0.3,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"patience\": 12,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 4,\n",
    "    \"seed\": 42,\n",
    "    \"n_splits\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f84a169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN2D_LSTM_Net_fn = lambda: CNN2D_LSTM_Net(\n",
    "    num_classes=CONFIG[\"num_classes\"],\n",
    "    input_sensors=CONFIG[\"input_sensors\"],\n",
    "    input_time_steps=CONFIG[\"input_time_steps\"],\n",
    "    lstm_hidden_size=CONFIG[\"lstm_hidden_size\"],\n",
    "    num_lstm_layers=CONFIG[\"num_lstm_layers\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d26f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  CROSS-SUBJECT TRAINING\n",
      "============================================================\n",
      "Training data path: c:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\Final Project data\\Final Project data\\Cross\\train\n",
      "File batch size: 8\n",
      "DataLoader batch size: 4\n",
      "Downsample factor: 4\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Input shape: (248, 8906)\n",
      "2D CNN + LSTM Net initialized:\n",
      "  - LSTM input size: 7936\n",
      "  - Sequence length: 2227\n",
      "  - Parameters: 1,032,180\n",
      "\n",
      "Starting training for 50 epochs...\n",
      "Found 64 files in c:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\Final Project data\\Final Project data\\Cross\\train\n",
      "Epoch   0: Train Acc=0.1667, Val Acc=0.2500, Train Loss=0.4690, Val Loss=0.7086\n",
      "Found 64 files in c:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\Final Project data\\Final Project data\\Cross\\train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, losses, accs, test_results = \u001b[43mrun_complete_cross_subject_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCNN2D_LSTM_Net_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLABEL_MAP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\DL_2\\Utils\\utilities_Cross.py:605\u001b[39m, in \u001b[36mrun_complete_cross_subject_experiment\u001b[39m\u001b[34m(filepath, model_fn, label_map, epochs, file_batch_size, dataloader_batch_size, downsample_factor, lr, weight_decay, device)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[33;03mRun complete cross-subject experiment: train + evaluate + analyze\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m model, train_losses, train_accs, val_losses, val_accs = \u001b[43mtrain_cross_subject_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLABEL_MAP\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# Evaluate on test sets\u001b[39;00m\n\u001b[32m    619\u001b[39m model_path = \u001b[33m'\u001b[39m\u001b[33mbest_cnn_lstm_model.pth\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\DL_2\\Utils\\utilities_Cross.py:330\u001b[39m, in \u001b[36mtrain_cross_subject_model\u001b[39m\u001b[34m(filepath, model_fn, LABEL_MAP, epochs, device, file_batch_size, dataloader_batch_size, downsample_factor, lr, weight_decay)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     train_loss, train_acc, val_loss, val_acc = \u001b[43mtrain_one_epoch_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_train_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABEL_MAP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     train_losses.append(train_loss)\n\u001b[32m    338\u001b[39m     train_accs.append(train_acc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\DL_2\\Utils\\utilities_Cross.py:161\u001b[39m, in \u001b[36mtrain_one_epoch_batched\u001b[39m\u001b[34m(model, train_path, label_map, criterion, optimizer, batch_size, downsample_factor, dataloader_batch_size, device, verbose)\u001b[39m\n\u001b[32m    157\u001b[39m total_val_samples = \u001b[32m0\u001b[39m\n\u001b[32m    159\u001b[39m batch_count = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_and_normalize_files_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    163\u001b[39m \n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Split batch into train and validation\u001b[39;49;00m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# === Train ===\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\DL_2\\Utils\\utilities_Cross.py:119\u001b[39m, in \u001b[36mload_and_normalize_files_batch\u001b[39m\u001b[34m(directory_path, batch_size, downsample_factor, verbose)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Load and process the data\u001b[39;00m\n\u001b[32m    118\u001b[39m file_path = os.path.join(directory_path, file)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m matrix = \u001b[43mread_h5py_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Downsample if specified\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m downsample_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marcd\\Desktop\\Master\\Courses\\Deep_Learning\\Project_2\\DL_2\\Utils\\utilities_Cross.py:58\u001b[39m, in \u001b[36mread_h5py_file\u001b[39m\u001b[34m(filename_path)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(filename_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     57\u001b[39m     dataset_name = get_dataset_name(filename_path)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     matrix = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# print(f\"Loaded: {type(matrix)}, Shape: {matrix.shape}\")\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py\\\\_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py\\\\_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marcd\\anaconda3\\envs\\DL_Project\\Lib\\site-packages\\h5py\\_hl\\dataset.py:781\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    783\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model, losses, accs, test_results = run_complete_cross_subject_experiment(\n",
    "    filepath=filepath,\n",
    "    model_fn=CNN2D_LSTM_Net_fn,\n",
    "    label_map=LABEL_MAP,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    epochs=50,\n",
    "    file_batch_size=8,\n",
    "    dataloader_batch_size=4,\n",
    "    downsample_factor=4,\n",
    "    lr=1e-3,               \n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
