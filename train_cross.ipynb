{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5868605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base filepath: /Users/jesseh/Library/Mobile Documents/com~apple~CloudDocs/AA Master AI/Deep Learning\n",
      "H5 file path: /Users/jesseh/Library/Mobile Documents/com~apple~CloudDocs/AA Master AI/Deep Learning/Intra/train/rest_105923_1.h5\n",
      "Data min: -2.788253829211218e-11, max: 1.3240070498299339e-11, mean: 4.094354904508632e-14\n",
      "Normalized data min: -6.443676015954523, max: 6.593805371269012, mean: 2.03831359330788e-17\n",
      "Mean of means: 2.3337982895599773e-17, should be close to 0\n",
      "Mean of stds: 1.0, should be close to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesseh/Library/Mobile Documents/com~apple~CloudDocs/AA Master AI/Deep Learning/DL_2/utilities.py:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  modified_path = localpath.replace('\\DL_2', '')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from utilities import *\n",
    "import h5py\n",
    "\n",
    "# Use os.path.join() to create the correct file path\n",
    "filepath = get_filepath()\n",
    "print(f\"Base filepath: {filepath}\")\n",
    "\n",
    "# Path to the specific h5 file\n",
    "h5_filepath = os.path.join(filepath, \"Intra\", \"train\", \"rest_105923_1.h5\")\n",
    "print(f\"H5 file path: {h5_filepath}\")\n",
    "\n",
    "# Load the data\n",
    "data = read_h5py_file(h5_filepath)\n",
    "\n",
    "# Check the data properties\n",
    "print(f\"Data min: {np.min(data)}, max: {np.max(data)}, mean: {np.mean(data)}\")\n",
    "\n",
    "# Z-score normalization (time-wise normalization as suggested in the document)\n",
    "# This normalizes each sensor's time series independently\n",
    "def z_score_normalize(data):\n",
    "    \"\"\"\n",
    "    Perform z-score normalization on the data time-wise (for each sensor)\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): Data with shape (n_sensors, n_timepoints)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized data with the same shape\n",
    "    \"\"\"\n",
    "    # Calculate mean and std for each sensor (row)\n",
    "    mean = np.mean(data, axis=1, keepdims=True)\n",
    "    std = np.std(data, axis=1, keepdims=True)\n",
    "    \n",
    "    # Replace zero std with 1 to avoid division by zero\n",
    "    std[std == 0] = 1.0\n",
    "    \n",
    "    # Z-score normalization: (x - mean) / std\n",
    "    normalized_data = (data - mean) / std\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "# Apply z-score normalization\n",
    "normalized_data = z_score_normalize(data)\n",
    "\n",
    "# Check the normalized data properties\n",
    "print(f\"Normalized data min: {np.min(normalized_data)}, max: {np.max(normalized_data)}, mean: {np.mean(normalized_data)}\")\n",
    "\n",
    "# Verify that the normalization worked as expected\n",
    "# After z-score normalization, each row should have mean ≈ 0 and std ≈ 1\n",
    "means = np.mean(normalized_data, axis=1)\n",
    "stds = np.std(normalized_data, axis=1)\n",
    "print(f\"Mean of means: {np.mean(means)}, should be close to 0\")\n",
    "print(f\"Mean of stds: {np.mean(stds)}, should be close to 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae88f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 32 training files and 8 test files\n",
      "Training labels: ['rest', 'task_story', 'task_motor', 'task_story', 'task_story', 'task_working', 'task_motor', 'rest', 'rest', 'task_working', 'task_motor', 'rest', 'task_motor', 'task_working', 'task_motor', 'rest', 'rest', 'task_motor', 'task_working', 'task_story', 'task_motor', 'task_working', 'task_motor', 'task_working', 'rest', 'rest', 'task_working', 'task_story', 'task_story', 'task_story', 'task_working', 'task_story']\n",
      "Test labels: ['task_motor', 'rest', 'task_story', 'task_working', 'task_motor', 'task_story', 'task_working', 'rest']\n",
      "Shape of first training sample after downsampling: (248, 2227)\n"
     ]
    }
   ],
   "source": [
    "def load_and_normalize_files(directory_path, max_files=None, downsample_factor=None):\n",
    "    \"\"\"\n",
    "    Load and normalize all h5 files in the specified directory\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing h5 files\n",
    "    max_files (int, optional): Maximum number of files to load\n",
    "    downsample_factor (int, optional): Factor by which to downsample the data\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (data, labels) where data is a list of normalized matrices and labels are the corresponding task types\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get all h5 files in the directory\n",
    "    h5_files = [f for f in os.listdir(directory_path) if f.endswith('.h5')]\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        h5_files = h5_files[:max_files]\n",
    "    \n",
    "    for file in h5_files:\n",
    "       # Extract the task type from the filename\n",
    "        if file.startswith(\"task_\"):\n",
    "            parts = file.split('_')\n",
    "            task_type = '_'.join(parts[:2])  # e.g., task_motor or task_working\n",
    "        else:\n",
    "            task_type = file.split('_')[0]  # e.g., rest\n",
    "\n",
    "        # Load the data\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        matrix = read_h5py_file(file_path)\n",
    "        \n",
    "        # Downsample if specified\n",
    "        if downsample_factor is not None:\n",
    "            matrix = matrix[:, ::downsample_factor]\n",
    "        \n",
    "        # Normalize the data\n",
    "        normalized_matrix = z_score_normalize(matrix)\n",
    "        \n",
    "        # Add to lists\n",
    "        data_list.append(normalized_matrix)\n",
    "        labels.append(task_type)\n",
    "    \n",
    "    return data_list, labels\n",
    "\n",
    "# Example usage for Intra-subject classification\n",
    "intra_train_path = os.path.join(filepath, \"Intra\", \"train\")\n",
    "intra_test_path = os.path.join(filepath, \"Intra\", \"test\")\n",
    "\n",
    "# Load a small subset of files to test the function\n",
    "# Downsample factor is set to 16 to speed up the process, CHANGE LATER!\n",
    "train_data, train_labels = load_and_normalize_files(intra_train_path, downsample_factor=16)\n",
    "test_data, test_labels = load_and_normalize_files(intra_test_path, downsample_factor=16)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nLoaded {len(train_data)} training files and {len(test_data)} test files\")\n",
    "print(f\"Training labels: {train_labels}\")\n",
    "print(f\"Test labels: {test_labels}\")\n",
    "print(f\"Shape of first training sample after downsampling: {train_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06a571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676be6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
