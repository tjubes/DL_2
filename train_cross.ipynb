{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install scipy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5868605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base filepath: /Users/jesseh/Library/Mobile Documents/com~apple~CloudDocs/AA Master AI/Deep Learning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from utilities import *\n",
    "import h5py\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Use os.path.join() to create the correct file path\n",
    "filepath = get_filepath()\n",
    "print(f\"Base filepath: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ae88f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: {'task_story', 'task_working', 'task_motor', 'rest'}\n",
      "\n",
      "Loaded 32 training files and 8 test files\n",
      "Training labels: ['rest', 'task_story', 'task_motor', 'task_story', 'task_story', 'task_working', 'task_motor', 'rest', 'rest', 'task_working', 'task_motor', 'rest', 'task_motor', 'task_working', 'task_motor', 'rest', 'rest', 'task_motor', 'task_working', 'task_story', 'task_motor', 'task_working', 'task_motor', 'task_working', 'rest', 'rest', 'task_working', 'task_story', 'task_story', 'task_story', 'task_working', 'task_story']\n",
      "Test labels: ['task_motor', 'rest', 'task_story', 'task_working', 'task_motor', 'task_story', 'task_working', 'rest']\n",
      "Shape of first training sample after downsampling: (248, 2227)\n"
     ]
    }
   ],
   "source": [
    "def load_and_normalize_files(directory_path, max_files=None, downsample_factor=None):\n",
    "    \"\"\"\n",
    "    Load and normalize all h5 files in the specified directory\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing h5 files\n",
    "    max_files (int, optional): Maximum number of files to load\n",
    "    downsample_factor (int, optional): Factor by which to downsample the data\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (data, labels) where data is a list of normalized matrices and labels are the corresponding task types\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get all h5 files in the directory\n",
    "    h5_files = [f for f in os.listdir(directory_path) if f.endswith('.h5')]\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        h5_files = h5_files[:max_files]\n",
    "    \n",
    "    for file in h5_files:\n",
    "       # Extract the task type from the filename\n",
    "        if file.startswith(\"task_\"):\n",
    "            parts = file.split('_')\n",
    "            task_type = '_'.join(parts[:2])  # e.g., task_motor or task_working\n",
    "        else:\n",
    "            task_type = file.split('_')[0]  # e.g., rest\n",
    "\n",
    "        # Load the data\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        matrix = read_h5py_file(file_path)\n",
    "        \n",
    "        # Downsample if specified\n",
    "        if downsample_factor is not None:\n",
    "            matrix = matrix[:, ::downsample_factor]\n",
    "        \n",
    "        # Normalize the data using scipy's zscore\n",
    "        normalized_matrix = zscore(matrix, axis=1, nan_policy='propagate')\n",
    "        normalized_matrix = np.nan_to_num(normalized_matrix, nan=0.0)\n",
    "        \n",
    "        # Add to lists\n",
    "        data_list.append(normalized_matrix)\n",
    "        labels.append(task_type)\n",
    "    \n",
    "    return data_list, labels\n",
    "\n",
    "# Example usage for Intra-subject classification\n",
    "intra_train_path = os.path.join(filepath, \"Intra\", \"train\")\n",
    "intra_test_path = os.path.join(filepath, \"Intra\", \"test\")\n",
    "\n",
    "# Load a small subset of files to test the function\n",
    "# Downsample factor is set to 16 to speed up the process, CHANGE LATER!\n",
    "train_data, train_labels = load_and_normalize_files(intra_train_path, downsample_factor=16)\n",
    "test_data, test_labels = load_and_normalize_files(intra_test_path, downsample_factor=16)\n",
    "unique_labels = set(train_labels + test_labels)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nLoaded {len(train_data)} training files and {len(test_data)} test files\")\n",
    "print(f\"Training labels: {train_labels}\")\n",
    "print(f\"Test labels: {test_labels}\")\n",
    "print(f\"Shape of first training sample after downsampling: {train_data[0].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a simple CNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676be6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Alternative: You might also consider Adam optimizer (optim.Adam()) which adapts learning rates \n",
    "# automatically and often works well for neural networks without needing to tune momentum manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
