{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6010724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scipy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5868605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base filepath: /Users/jesseh/Library/Mobile Documents/com~apple~CloudDocs/AA Master AI/Deep Learning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from utilities import *\n",
    "import h5py\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Use os.path.join() to create the correct file path\n",
    "filepath = get_filepath()\n",
    "print(f\"Base filepath: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae88f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: {'task_motor', 'task_story', 'task_working', 'rest'}\n",
      "\n",
      "Loaded 32 training files and 8 test files\n",
      "Training labels: ['rest', 'task_story', 'task_motor', 'task_story', 'task_story', 'task_working', 'task_motor', 'rest', 'rest', 'task_working', 'task_motor', 'rest', 'task_motor', 'task_working', 'task_motor', 'rest', 'rest', 'task_motor', 'task_working', 'task_story', 'task_motor', 'task_working', 'task_motor', 'task_working', 'rest', 'rest', 'task_working', 'task_story', 'task_story', 'task_story', 'task_working', 'task_story']\n",
      "Test labels: ['task_motor', 'rest', 'task_story', 'task_working', 'task_motor', 'task_story', 'task_working', 'rest']\n",
      "Shape of first training sample after downsampling: (248, 2227)\n"
     ]
    }
   ],
   "source": [
    "def load_and_normalize_files(directory_path, max_files=None, downsample_factor=None):\n",
    "    \"\"\"\n",
    "    Load and normalize all h5 files in the specified directory\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing h5 files\n",
    "    max_files (int, optional): Maximum number of files to load\n",
    "    downsample_factor (int, optional): Factor by which to downsample the data\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (data, labels) where data is a list of normalized matrices and labels are the corresponding task types\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get all h5 files in the directory\n",
    "    h5_files = [f for f in os.listdir(directory_path) if f.endswith('.h5')]\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        h5_files = h5_files[:max_files]\n",
    "    \n",
    "    for file in h5_files:\n",
    "       # Extract the task type from the filename\n",
    "        if file.startswith(\"task_\"):\n",
    "            parts = file.split('_')\n",
    "            task_type = '_'.join(parts[:2])  # e.g., task_motor or task_working\n",
    "        else:\n",
    "            task_type = file.split('_')[0]  # e.g., rest\n",
    "\n",
    "        # Load the data\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        matrix = read_h5py_file(file_path)\n",
    "        \n",
    "        # Downsample if specified\n",
    "        if downsample_factor is not None:\n",
    "            matrix = matrix[:, ::downsample_factor]\n",
    "        \n",
    "        # Normalize the data using scipy's zscore\n",
    "        normalized_matrix = zscore(matrix, axis=1, nan_policy='propagate')\n",
    "        normalized_matrix = np.nan_to_num(normalized_matrix, nan=0.0)\n",
    "        \n",
    "        # Add to lists\n",
    "        data_list.append(normalized_matrix)\n",
    "        labels.append(task_type)\n",
    "    \n",
    "    return data_list, labels\n",
    "\n",
    "# Example usage for Intra-subject classification\n",
    "intra_train_path = os.path.join(filepath, \"Intra\", \"train\")\n",
    "intra_test_path = os.path.join(filepath, \"Intra\", \"test\")\n",
    "\n",
    "# Load a small subset of files to test the function\n",
    "# Downsample factor is set to 16 to speed up the process, CHANGE LATER!\n",
    "train_data, train_labels = load_and_normalize_files(intra_train_path, downsample_factor=16)\n",
    "test_data, test_labels = load_and_normalize_files(intra_test_path, downsample_factor=16)\n",
    "unique_labels = set(train_labels + test_labels)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nLoaded {len(train_data)} training files and {len(test_data)} test files\")\n",
    "print(f\"Training labels: {train_labels}\")\n",
    "print(f\"Test labels: {test_labels}\")\n",
    "print(f\"Shape of first training sample after downsampling: {train_data[0].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d06a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Standard class mapping for MEG tasks\n",
    "LABEL_MAP = {\n",
    "    'rest': 0,\n",
    "    'task_motor': 1, \n",
    "    'task_story': 2,\n",
    "    'task_working': 3\n",
    "}\n",
    "\n",
    "class MEGDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for MEG data\"\"\"\n",
    "    def __init__(self, data_list, labels):\n",
    "        self.data_list = data_list\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the data matrix (248, time_steps)\n",
    "        data = self.data_list[idx]\n",
    "        \n",
    "        # Add channel dimension: (248, time_steps) -> (1, 248, time_steps)\n",
    "        data = torch.FloatTensor(data).unsqueeze(0)\n",
    "        \n",
    "        # Convert string label to integer\n",
    "        label = LABEL_MAP[self.labels[idx]]\n",
    "        label = torch.LongTensor([label]).squeeze()\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "class MEGNet(nn.Module):\n",
    "    \"\"\"Smaller 2D CNN for MEG data classification\"\"\"\n",
    "    def __init__(self, num_classes=4, input_channels=248, input_time_steps=2227):\n",
    "        super(MEGNet, self).__init__()\n",
    "        \n",
    "        # Smaller conv blocks\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(8, 16), padding=(4, 8))\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(4, 8))  # More aggressive pooling\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(4, 8), padding=(2, 4))\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(4, 8))  # More aggressive pooling\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.flatten_size = self._calculate_flatten_size(input_channels, input_time_steps)\n",
    "        \n",
    "        # Smaller fully connected layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def _calculate_flatten_size(self, channels, time_steps):\n",
    "        \"\"\"Calculate the size after all conv+pool operations\"\"\"\n",
    "        # Simulate forward pass to get dimensions\n",
    "        x = torch.zeros(1, 1, channels, time_steps)\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        return x.numel()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv blocks with ReLU and pooling\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f504e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Total parameters: 1,063,380\n",
      "Model input shape expected: (batch, 1, 248, 2227)\n"
     ]
    }
   ],
   "source": [
    "net = MEGNet(num_classes=len(LABEL_MAP), \n",
    "             input_channels=248, \n",
    "             input_time_steps=train_data[0].shape[1])  # Use actual time steps after downsampling\n",
    "\n",
    "print(f\"Model created. Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "print(f\"Model input shape expected: (batch, 1, 248, {train_data[0].shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460cbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_data, train_labels, test_data, test_labels, batch_size=4):\n",
    "    \"\"\"Create PyTorch DataLoaders from your loaded data\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MEGDataset(train_data, train_labels)\n",
    "    test_dataset = MEGDataset(test_data, test_labels)\n",
    "    \n",
    "    # Create dataloaders with num_workers=0 to avoid multiprocessing issues\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42af684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "print(f\"Creating DataLoaders...\")\n",
    "trainloader, testloader = create_dataloaders(train_data, train_labels, test_data, test_labels, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676be6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Alternative: You might also consider Adam optimizer (optim.Adam()) which adapts learning rates \n",
    "# automatically and often works well for neural networks without needing to tune momentum manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aded1ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce79712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127820a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
